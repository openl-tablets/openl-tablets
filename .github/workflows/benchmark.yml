name: Performance Benchmark

on:
  # Only run benchmarks manually or on schedule to save CI resources
  # PRs can manually trigger if needed
  workflow_dispatch:
  schedule:
    # Run weekly on Sunday at 2 AM UTC
    - cron: '0 2 * * 0'
  push:
    branches:
      - main
    # Only run on main branch pushes, not develop or PRs

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: maven

      - name: Build project
        run: mvn clean install -DskipTests -T1C

      - name: Run JMH Benchmarks
        continue-on-error: true  # Don't fail if benchmarks don't exist or fail
        run: |
          # Run JMH benchmarks if they exist
          if [ -d "benchmarks" ]; then
            echo "Found benchmarks module, running JMH..."
            cd benchmarks
            mvn clean install
            java -jar target/benchmarks.jar -rf json -rff benchmark-results.json || {
              echo "Benchmark execution failed, but continuing..."
              echo '{"benchmarks":[]}' > benchmark-results.json
            }
          else
            echo "No benchmark module found, skipping JMH benchmarks"
            mkdir -p benchmark-results
            echo '{"benchmarks":[]}' > benchmark-results/benchmark-results.json
          fi

      - name: Run Rule Execution Benchmarks
        continue-on-error: true  # Don't fail if benchmarks fail
        run: |
          mkdir -p benchmark-results

          # Compilation benchmark
          echo "Running compilation benchmark..."
          if [ -d "DEV/org.openl.rules" ]; then
            time mvn clean compile -pl DEV/org.openl.rules 2>&1 | tee benchmark-results/compilation.log || {
              echo "Compilation benchmark failed" | tee benchmark-results/compilation.log
            }
          else
            echo "DEV/org.openl.rules not found, skipping compilation benchmark" | tee benchmark-results/compilation.log
          fi

          # Rule execution benchmark (if test rules exist)
          echo "Running rule execution benchmark..."
          mvn test -Dtest="*PerfTest,*BenchmarkTest" -DfailIfNoTests=false 2>&1 | tee benchmark-results/execution.log || {
            echo "No performance tests found or execution failed" | tee -a benchmark-results/execution.log
          }

      - name: Parse benchmark results
        run: |
          cat > benchmark-results/summary.md <<'EOF'
          # Performance Benchmark Results

          **Run**: ${{ github.run_number }}
          **Branch**: ${{ github.ref_name }}
          **Commit**: ${{ github.sha }}
          **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Compilation Performance

          ```
          EOF

          if [ -f benchmark-results/compilation.log ]; then
            grep "real" benchmark-results/compilation.log >> benchmark-results/summary.md || echo "N/A" >> benchmark-results/summary.md
          fi

          cat >> benchmark-results/summary.md <<'EOF'
          ```

          ## Rule Execution Performance

          ```
          EOF

          if [ -f benchmark-results/execution.log ]; then
            grep -E "(Tests run:|Time elapsed:)" benchmark-results/execution.log | head -10 >> benchmark-results/summary.md || echo "N/A" >> benchmark-results/summary.md
          fi

          echo '```' >> benchmark-results/summary.md

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        continue-on-error: true  # Don't fail if benchmark action fails
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
        with:
          name: OpenL Tablets Benchmarks
          tool: 'customBiggerIsBetter'
          output-file-path: benchmark-results/benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: benchmark-results/
          retention-days: 90

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        continue-on-error: true  # Don't fail if comment creation fails
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            if (!fs.existsSync('benchmark-results/summary.md')) {
              core.info('No benchmark summary found, skipping PR comment');
              return;
            }

            const summary = fs.readFileSync('benchmark-results/summary.md', 'utf8');

            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 30
    continue-on-error: true  # Don't fail overall workflow if profiling fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: maven

      - name: Build project
        run: mvn clean install -DskipTests

      - name: Run with memory profiling
        continue-on-error: true
        run: |
          mkdir -p profiling-results

          # Check if webapp exists
          if [ ! -f "STUDIO/org.openl.rules.webstudio/target/webapp.war" ]; then
            echo "Webapp JAR not found, skipping memory profiling" | tee profiling-results/skip.log
            exit 0
          fi

          # Run with JFR (Java Flight Recorder)
          java -XX:StartFlightRecording=duration=60s,filename=profiling-results/recording.jfr \
               -jar STUDIO/org.openl.rules.webstudio/target/webapp.war &
          PID=$!

          # Wait for startup
          sleep 30

          # Simulate some activity (if endpoints are available)
          curl -f http://localhost:8080/actuator/health || echo "Health check failed or endpoint unavailable"

          # Wait for completion
          wait $PID || echo "Webapp execution completed or failed"

      - name: Analyze memory usage
        run: |
          # Create memory analysis report
          cat > profiling-results/memory-analysis.md <<'EOF'
          # Memory Profiling Results

          **Run**: ${{ github.run_number }}
          **Branch**: ${{ github.ref_name }}
          **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## JFR Recording

          A Java Flight Recording was captured during a 60-second run.

          ### Analysis

          To analyze the recording locally:

          ```bash
          # Download the artifact
          # Extract recording.jfr
          # Open in JDK Mission Control (JMC)
          jmc recording.jfr
          ```

          ### Key Metrics to Check

          - Heap usage over time
          - GC pause times
          - Object allocation rate
          - Thread activity
          - Method profiling (hot methods)

          EOF

      - name: Upload profiling artifacts
        uses: actions/upload-artifact@v4
        with:
          name: profiling-results-${{ github.run_number }}
          path: profiling-results/
          retention-days: 30

  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 45
    continue-on-error: true  # Don't fail overall workflow if load testing fails
    # Only run on manual trigger or schedule, not on every push
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: openl_test
          POSTGRES_USER: openl
          POSTGRES_PASSWORD: openl
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: maven

      - name: Build and start Rule Service
        continue-on-error: true
        run: |
          mvn clean install -DskipTests -T1C

          # Check if webapp exists
          if [ ! -f "WSFrontend/org.openl.rules.ruleservice.ws/target/webapp.war" ]; then
            echo "Rule Service webapp not found, skipping load test"
            exit 0
          fi

          java -jar WSFrontend/org.openl.rules.ruleservice.ws/target/webapp.war \
            --spring.datasource.url=jdbc:postgresql://localhost:5432/openl_test \
            --spring.datasource.username=openl \
            --spring.datasource.password=openl &
          RULE_SERVICE_PID=$!
          echo "RULE_SERVICE_PID=$RULE_SERVICE_PID" >> $GITHUB_ENV

          # Wait for startup
          for i in {1..30}; do
            if curl -f http://localhost:8080/actuator/health 2>/dev/null; then
              echo "Rule Service started successfully"
              break
            fi
            echo "Waiting for Rule Service to start... ($i/30)"
            sleep 2
          done

      - name: Install k6
        continue-on-error: true
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69 || {
            echo "Failed to add k6 GPG key, skipping k6 installation"
            exit 0
          }
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6 || echo "Failed to install k6"

      - name: Run load tests
        continue-on-error: true
        run: |
          mkdir -p load-test-results

          # Check if k6 is installed
          if ! command -v k6 &> /dev/null; then
            echo "k6 not installed, skipping load tests" | tee load-test-results/skip.log
            exit 0
          fi

          # Create k6 test script
          cat > load-test.js <<'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';

          export const options = {
            stages: [
              { duration: '1m', target: 10 },  // Ramp up to 10 users
              { duration: '3m', target: 10 },  // Stay at 10 users
              { duration: '1m', target: 50 },  // Ramp up to 50 users
              { duration: '3m', target: 50 },  // Stay at 50 users
              { duration: '1m', target: 0 },   // Ramp down
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'], // 95% of requests under 500ms
              http_req_failed: ['rate<0.01'],   // Less than 1% failure rate
            },
          };

          export default function () {
            const res = http.get('http://localhost:8080/actuator/health');
            check(res, {
              'status is 200': (r) => r.status === 200,
            });
            sleep(1);
          }
          EOF

          # Run k6 test
          k6 run --out json=load-test-results/results.json load-test.js || true

      - name: Generate load test report
        run: |
          cat > load-test-results/report.md <<'EOF'
          # Load Test Results

          **Run**: ${{ github.run_number }}
          **Branch**: ${{ github.ref_name }}
          **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Test Configuration

          - Duration: 9 minutes
          - Max concurrent users: 50
          - Ramp-up: Gradual (10 â†’ 50)

          ## Results

          See `results.json` for detailed metrics.

          ### Key Metrics

          Check the artifact for:
          - Request rate (RPS)
          - Response times (p95, p99)
          - Error rate
          - Throughput

          EOF

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results-${{ github.run_number }}
          path: load-test-results/
          retention-days: 90

      - name: Cleanup
        if: always()
        run: |
          if [ -n "$RULE_SERVICE_PID" ]; then
            kill $RULE_SERVICE_PID || true
          fi
